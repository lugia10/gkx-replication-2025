{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GKY (2020) Replication: Part 4 - Model Training & Evaluation\n",
    "\n",
    "##### This is the main workhorse notebook for the GKY replication project. It implements the complete training, tuning, and out-of-sample (OOS) prediction loop.\n",
    "\n",
    "##### It is structured around an object-oriented design:\n",
    "##### 1.  **A `BaseModel` Class:** This class acts as a generic \"engine\" that handles the entire evaluation process:\n",
    "##### - It manages the annual looping through training, validation, and testing periods.\n",
    "##### - It uses `GridSearchCV` with `PredefinedSplit` to correctly tune hyperparameters on the validation set without any data leakage.\n",
    "##### - It collects all OOS predictions and calculates the final R²_OOS metric as defined in the paper.\n",
    "\n",
    "##### 2.  **Specific Model Subclasses:** Each machine learning model (e.g., PCR, Random Forest) is a simple class that inherits from `BaseModel`. Its only job is to provide its specific scikit-learn estimator and hyperparameter grid, making the code clean, modular, and easy to extend.\n",
    "\n",
    "##### 3.  **Execution Block:** The final section instantiates each model object and runs the full evaluation, collecting the results to replicate Table 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Scikit-learn Imports ---\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Constants\n",
    "\n",
    "FIRST_OOS_YEAR = 1987\n",
    "LAST_OOS_YEAR = 2016\n",
    "TRAINING_START_YEAR = 1957\n",
    "VALIDATION_LENGTH_YEARS = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Setup and Framework ---\n",
      "Loading final dataset from './data/gky_final_data.parquet'...\n",
      "Data loaded successfully. Shape: (2851604, 923)\n"
     ]
    }
   ],
   "source": [
    "# --- Part 1: Setup and Framework ---\n",
    "print(\"--- Part 1: Setup and Framework ---\")\n",
    "\n",
    "# Define file paths and constants\n",
    "INPUT_DIR = './data'\n",
    "INPUT_PATH = os.path.join(INPUT_DIR, 'gky_final_data.parquet')\n",
    "\n",
    "# Load the final analysis-ready dataset\n",
    "print(f\"Loading final dataset from '{INPUT_PATH}'...\")\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(\"Final data file not found. Please run 01 and 02 notebooks first.\")\n",
    "gky_final_data = pd.read_parquet(INPUT_PATH)\n",
    "gky_final_data['month'] = pd.to_datetime(gky_final_data['month'])\n",
    "print(f\"Data loaded successfully. Shape: {gky_final_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation periods framework created.\n",
      "   oos_year  train_start  train_end  valid_start  valid_end\n",
      "0      1987         1957       1974         1975       1986\n",
      "1      1988         1957       1975         1976       1987\n",
      "2      1989         1957       1976         1977       1988\n",
      "Identified 919 predictor columns.\n"
     ]
    }
   ],
   "source": [
    "# Create the evaluation periods DataFrame\n",
    "periods = []\n",
    "for year in range(FIRST_OOS_YEAR, LAST_OOS_YEAR + 1):\n",
    "    periods.append({\n",
    "        'oos_year': year,\n",
    "        'train_start': TRAINING_START_YEAR,\n",
    "        'train_end': year - VALIDATION_LENGTH_YEARS - 1,\n",
    "        'valid_start': year - VALIDATION_LENGTH_YEARS,\n",
    "        'valid_end': year - 1,\n",
    "    })\n",
    "evaluation_periods_df = pd.DataFrame(periods)\n",
    "print(\"Evaluation periods framework created.\")\n",
    "print(evaluation_periods_df.head(3))\n",
    "\n",
    "# Define predictor columns once\n",
    "ID_COLS = ['permno', 'month', 'ret_excess', 'mktcap_lag']\n",
    "PREDICTOR_COLS = [col for col in gky_final_data.columns if col not in ID_COLS]\n",
    "print(f\"Identified {len(PREDICTOR_COLS)} predictor columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 2: Defining the Core `BaseModel` Engine ---\n",
      "BaseModel class defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Part 2: The Core Modeling Engine (`BaseModel` Class) ---\n",
    "print(\"\\n--- Part 2: Defining the Core `BaseModel` Engine ---\")\n",
    "\n",
    "class BaseModel:\n",
    "    \"\"\"\n",
    "    A generic base class for handling the GKY (2020) evaluation methodology.\n",
    "    \n",
    "    This class orchestrates the annual training, validation, and testing loop,\n",
    "    performs hyperparameter tuning using GridSearchCV with a PredefinedSplit,\n",
    "    and calculates the final out-of-sample R-squared.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, estimator: BaseEstimator, param_grid: dict, predictor_subset: list = None):\n",
    "        self.model_name = model_name\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.predictor_subset = predictor_subset if predictor_subset else PREDICTOR_COLS\n",
    "        self.all_oos_predictions = []\n",
    "        \n",
    "    def _train_and_predict_one_year(self, full_df: pd.DataFrame, period: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handles the model training, tuning, and prediction for a single OOS year.\n",
    "        \"\"\"\n",
    "        # 1. Slice data into train, validation, and OOS test sets\n",
    "        train_df = full_df[\n",
    "            (full_df['month'].dt.year >= period.train_start) & \n",
    "            (full_df['month'].dt.year <= period.train_end)\n",
    "        ]\n",
    "        valid_df = full_df[\n",
    "            (full_df['month'].dt.year >= period.valid_start) & \n",
    "            (full_df['month'].dt.year <= period.valid_end)\n",
    "        ]\n",
    "        oos_df = full_df[full_df['month'].dt.year == period.oos_year]\n",
    "\n",
    "        # 2. Prepare data for scikit-learn\n",
    "        X_train, y_train = train_df[self.predictor_subset], train_df['ret_excess']\n",
    "        X_valid, y_valid = valid_df[self.predictor_subset], valid_df['ret_excess']\n",
    "        X_oos,   y_oos   = oos_df[self.predictor_subset],   oos_df['ret_excess']\n",
    "        \n",
    "        X_tune = pd.concat([X_train, X_valid], ignore_index=True)\n",
    "        y_tune = pd.concat([y_train, y_valid], ignore_index=True)\n",
    "\n",
    "        # 3. Set up PredefinedSplit for GridSearchCV\n",
    "        # -1 for training samples, 0 for validation samples\n",
    "        split_index = [-1] * len(X_train) + [0] * len(X_valid)\n",
    "        pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "        # 4. Tune hyperparameters\n",
    "        tuner = GridSearchCV(\n",
    "            estimator=self.estimator,\n",
    "            param_grid=self.param_grid,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=pds,\n",
    "            n_jobs=-1,  # Use all available CPU cores\n",
    "            verbose=0   # Set to 1 or 2 for detailed tuning logs\n",
    "        )\n",
    "        tuner.fit(X_tune, y_tune)\n",
    "        \n",
    "        # 5. Make predictions on the OOS set\n",
    "        best_model = tuner.best_estimator_\n",
    "        predictions = best_model.predict(X_oos)\n",
    "\n",
    "        # 6. Store predictions with identifiers\n",
    "        oos_results = oos_df[ID_COLS].copy()\n",
    "        oos_results['prediction'] = predictions\n",
    "        \n",
    "        return oos_results\n",
    "\n",
    "    def _calculate_oos_r2(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the OOS R-squared based on the paper's formula.\n",
    "        \"\"\"\n",
    "        if not self.all_oos_predictions:\n",
    "            raise ValueError(\"No OOS predictions found. `run_full_evaluation` must be called first.\")\n",
    "        \n",
    "        preds_df = pd.concat(self.all_oos_predictions, ignore_index=True)\n",
    "        \n",
    "        sse = ((preds_df['ret_excess'] - preds_df['prediction']) ** 2).sum()\n",
    "        sst = (preds_df['ret_excess'] ** 2).sum() # Note: Denominator is not variance\n",
    "        \n",
    "        if sst == 0: return -np.inf # Avoid division by zero\n",
    "        \n",
    "        r2_oos = 1 - (sse / sst)\n",
    "        return r2_oos * 100 # Return as a percentage\n",
    "\n",
    "    def run_full_evaluation(self, full_df: pd.DataFrame, periods_df: pd.DataFrame) -> tuple[float, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Executes the full evaluation loop over all specified periods.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Starting Full Evaluation for: {self.model_name} ---\")\n",
    "        self.all_oos_predictions = [] # Reset predictions\n",
    "        \n",
    "        for _, period in tqdm(periods_df.iterrows(), total=len(periods_df), desc=self.model_name):\n",
    "            try:\n",
    "                oos_preds_for_year = self._train_and_predict_one_year(full_df, period)\n",
    "                self.all_oos_predictions.append(oos_preds_for_year)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during {self.model_name} evaluation for OOS year {period.oos_year}: {e}\")\n",
    "            gc.collect() # Clean memory after each intensive year\n",
    "            \n",
    "        final_r2 = self._calculate_oos_r2()\n",
    "        print(f\"--- Evaluation Complete for: {self.model_name} | Final OOS R² = {final_r2:.4f}% ---\")\n",
    "        \n",
    "        return final_r2, pd.concat(self.all_oos_predictions, ignore_index=True)\n",
    "\n",
    "print(\"BaseModel class defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom GKYHuberRegressor class defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Custom Estimator for GKY's Huber Loss Methodology ---\n",
    "class GKYHuberRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    A custom scikit-learn estimator that faithfully replicates the GKY (2020)\n",
    "    methodology for setting the Huber loss epsilon (ξ).\n",
    "    \n",
    "    The paper specifies epsilon as the 99.9% quantile of the absolute residuals.\n",
    "    This estimator performs a two-stage fit:\n",
    "    1. A preliminary OLS fit is used to generate residuals.\n",
    "    2. The 99.9% quantile of these absolute residuals is calculated.\n",
    "    3. The final HuberRegressor is fit using this dynamically determined epsilon.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_iter=500):\n",
    "        # max_iter is kept as a practical parameter to prevent convergence issues.\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon_ = None # Will store the calculated epsilon\n",
    "        self.final_huber_ = None # Will store the final fitted model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Stage 1: Preliminary fit to get residuals\n",
    "        # Using a simple OLS is fast and effective for this purpose.\n",
    "        prelim_model = LinearRegression(n_jobs=-1).fit(X, y)\n",
    "        residuals = y - prelim_model.predict(X)\n",
    "        \n",
    "        # Stage 2: Calculate the dynamic epsilon as per the paper\n",
    "        calculated_epsilon = np.quantile(np.abs(residuals), 0.999)\n",
    "        \n",
    "        # Stage 3: Adapt to scikit-learn's constraint (epsilon >= 1.0)\n",
    "        # This is the most faithful adaptation possible.\n",
    "        self.epsilon_ = max(1.0, calculated_epsilon)\n",
    "        \n",
    "        # Stage 4: Fit the final HuberRegressor with the correct, valid epsilon\n",
    "        self.final_huber_ = HuberRegressor(\n",
    "            epsilon=self.epsilon_, \n",
    "            max_iter=self.max_iter\n",
    "        ).fit(X, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.final_huber_ is None:\n",
    "            raise RuntimeError(\"You must fit the model before making predictions.\")\n",
    "        return self.final_huber_.predict(X)\n",
    "\n",
    "print(\"Custom GKYHuberRegressor class defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 3: Defining Specific Model Implementations ---\n",
      "All model classes defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Part 3: Model Definitions (Subclasses) ---\n",
    "print(\"\\n--- Part 3: Defining Specific Model Implementations ---\")\n",
    "\n",
    "# --- Linear Models ---\n",
    "class OLS3HModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        predictors = [\n",
    "            'characteristic_mvel1_x_macro_intercept', \n",
    "            'characteristic_bm_x_macro_intercept', \n",
    "            'characteristic_mom12m_x_macro_intercept'\n",
    "        ]\n",
    "        # This pipeline now uses our custom, faithful GKYHuberRegressor.\n",
    "        # Scaling is still critical for performance and convergence.\n",
    "        estimator = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('reg', GKYHuberRegressor(max_iter=500))\n",
    "        ])\n",
    "        param_grid = {} # No hyperparameters to tune\n",
    "        super().__init__(model_name=\"OLS-3+H\", estimator=estimator, param_grid=param_grid, predictor_subset=predictors)\n",
    "\n",
    "class OLSHModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        # Using the custom GKYHuberRegressor for the full model as well.\n",
    "        estimator = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('reg', GKYHuberRegressor(max_iter=500))\n",
    "        ])\n",
    "        param_grid = {} # No hyperparameters to tune\n",
    "        super().__init__(model_name=\"OLS+H\", estimator=estimator, param_grid=param_grid)\n",
    "        \n",
    "class PCRModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        estimator = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('reg', LinearRegression())\n",
    "        ])\n",
    "        # Grid based on GKY Figure 3, where K is often 20-40.\n",
    "        param_grid = {'pca__n_components': [10, 20, 30, 40, 50, 60]}\n",
    "        super().__init__(model_name=\"PCR\", estimator=estimator, param_grid=param_grid)\n",
    "\n",
    "class ENetHModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        # NOTE: scikit-learn's ElasticNet does not support Huber loss directly.\n",
    "        # This implementation uses the standard ElasticNet as the closest available\n",
    "        # off-the-shelf model, as allowed by the assignment. A full replication\n",
    "        # of the \"+H\" would require a different library.\n",
    "        estimator = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('reg', ElasticNet(l1_ratio=0.5, random_state=42, max_iter=1000))\n",
    "        ])\n",
    "        # Grid based on GKY Table A.5\n",
    "        param_grid = {'reg__alpha': np.logspace(-4, -1, 4)}\n",
    "        super().__init__(model_name=\"ENet+H\", estimator=estimator, param_grid=param_grid)\n",
    "\n",
    "# --- Tree-Based Models ---\n",
    "class RFModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        estimator = RandomForestRegressor(\n",
    "            n_estimators=300,\n",
    "            random_state=42,\n",
    "            n_jobs=-1 # Use all cores for tree building\n",
    "        )\n",
    "        # Grid based on GKY Table A.5 and TidyFinance post\n",
    "        param_grid = {\n",
    "            'max_features': [10, 20, 30, 50],\n",
    "            'min_samples_leaf': [5000, 10000] # Controls tree depth\n",
    "        }\n",
    "        super().__init__(model_name=\"RF\", estimator=estimator, param_grid=param_grid)\n",
    "\n",
    "# --- Neural Network Placeholders ---\n",
    "# These will be implemented using a deep learning library in the next step.\n",
    "class NN2Model(BaseModel):\n",
    "     def __init__(self):\n",
    "        # This is a placeholder and will not run correctly until implemented.\n",
    "        super().__init__(model_name=\"NN2\", estimator=None, param_grid={})\n",
    "\n",
    "class NN4Model(BaseModel):\n",
    "     def __init__(self):\n",
    "        # This is a placeholder and will not run correctly until implemented.\n",
    "        super().__init__(model_name=\"NN4\", estimator=None, param_grid={})\n",
    "\n",
    "print(\"All model classes defined successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 4: Running the Evaluation ---\n",
      "\n",
      "--- Starting Full Evaluation for: OLS+H ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OLS+H:   3%|▎         | 1/30 [01:46<51:14, 106.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR during OLS+H evaluation for OOS year 1987: \n",
      "All the 1 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/pipeline.py\", line 662, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/tmp/ipykernel_1927503/449136647.py\", line 32, in fit\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'epsilon' parameter of HuberRegressor must be a float in the range [1.0, inf). Got np.float64(0.7386611929541562) instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OLS+H:   7%|▋         | 2/30 [03:12<44:07, 94.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR during OLS+H evaluation for OOS year 1988: \n",
      "All the 1 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/pipeline.py\", line 662, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/tmp/ipykernel_1927503/449136647.py\", line 32, in fit\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'epsilon' parameter of HuberRegressor must be a float in the range [1.0, inf). Got np.float64(0.8520509753366518) instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OLS+H:  10%|█         | 3/30 [04:36<40:21, 89.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR during OLS+H evaluation for OOS year 1989: \n",
      "All the 1 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/pipeline.py\", line 662, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/tmp/ipykernel_1927503/449136647.py\", line 32, in fit\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'epsilon' parameter of HuberRegressor must be a float in the range [1.0, inf). Got np.float64(0.8762986094128198) instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OLS+H:  13%|█▎        | 4/30 [06:28<42:44, 98.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR during OLS+H evaluation for OOS year 1990: \n",
      "All the 1 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/pipeline.py\", line 662, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/tmp/ipykernel_1927503/449136647.py\", line 32, in fit\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'epsilon' parameter of HuberRegressor must be a float in the range [1.0, inf). Got np.float64(0.8806913527058152) instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Part 4: The Execution Block ---\n",
    "print(\"\\n--- Part 4: Running the Evaluation ---\")\n",
    "\n",
    "# NOTE: This block will take a significant amount of time to run,\n",
    "# especially for models with large hyperparameter grids.\n",
    "# It is recommended to run one or two models at a time to test.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Instantiate all models we want to evaluate for Table 1\n",
    "    models_to_run = {\n",
    "        \"OLS+H\": OLSHModel(),\n",
    "        \"OLS-3+H\": OLS3HModel(),\n",
    "        # \"PCR\": PCRModel(),\n",
    "        # \"ENet+H\": ENetHModel(),\n",
    "        # \"RF\": RFModel(),\n",
    "        # \"NN2\": NN2Model(), # Uncomment when implemented\n",
    "        # \"NN4\": NN4Model(), # Uncomment when implemented\n",
    "    }\n",
    "    \n",
    "    final_results = {}\n",
    "\n",
    "    for name, model_obj in models_to_run.items():\n",
    "        try:\n",
    "            r2_score, oos_preds = model_obj.run_full_evaluation(gky_final_data, evaluation_periods_df)\n",
    "            final_results[name] = r2_score\n",
    "            \n",
    "            # Optional: Save the detailed predictions for later analysis (e.g., Figure 9)\n",
    "            # preds_output_path = os.path.join(INPUT_DIR, f'preds_{name.replace(\"+\", \"\")}.parquet')\n",
    "            # oos_preds.to_parquet(preds_output_path, index=False)\n",
    "            # print(f\"Saved predictions for {name} to {preds_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! FAILED EVALUATION FOR {name}: {e} !!!\")\n",
    "            final_results[name] = 'Failed'\n",
    "    \n",
    "    # --- Display Final Table 1 Results ---\n",
    "    print(\"\\n\\n--- FINAL OOS R² RESULTS (Replication of Table 1) ---\\n\")\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(final_results, orient='index', columns=['R2_OOS (%)'])\n",
    "    results_df.index.name = \"Model\"\n",
    "    \n",
    "    # Format the DataFrame to match the paper's style\n",
    "    table1_output = pd.DataFrame(index=['All'])\n",
    "    for model_name in [\"OLS+H\", \"OLS-3+H\", \"PCR\", \"ENet+H\", \"RF\"]: # Add NNs here later\n",
    "        if model_name in results_df.index:\n",
    "            table1_output[model_name] = f\"{results_df.loc[model_name, 'R2_OOS (%)']:.2f}\"\n",
    "        else:\n",
    "            table1_output[model_name] = \"N/A\"\n",
    "            \n",
    "    print(table1_output)\n",
    "    print(\"\\nReplication of scikit-learn based models complete.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gkx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
