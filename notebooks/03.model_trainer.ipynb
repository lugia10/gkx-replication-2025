{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GKY (2020) Replication: Part 4 - Model Training & Evaluation\n",
    "\n",
    "##### This is the main workhorse notebook for the GKY replication project. It implements the complete training, tuning, and out-of-sample (OOS) prediction loop.\n",
    "\n",
    "##### It is structured around an object-oriented design:\n",
    "##### 1.  **A `BaseModel` Class:** This class acts as a generic \"engine\" that handles the entire evaluation process:\n",
    "##### - It manages the annual looping through training, validation, and testing periods.\n",
    "##### - It uses `GridSearchCV` with `PredefinedSplit` to correctly tune hyperparameters on the validation set without any data leakage.\n",
    "##### - It collects all OOS predictions and calculates the final R²_OOS metric as defined in the paper.\n",
    "\n",
    "##### 2.  **Specific Model Subclasses:** Each machine learning model (e.g., PCR, Random Forest) is a simple class that inherits from `BaseModel`. Its only job is to provide its specific scikit-learn estimator and hyperparameter grid, making the code clean, modular, and easy to extend.\n",
    "\n",
    "##### 3.  **Execution Block:** The final section instantiates each model object and runs the full evaluation, collecting the results to replicate Table 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Scikit-learn Imports ---\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Constants\n",
    "\n",
    "FIRST_OOS_YEAR = 1987\n",
    "LAST_OOS_YEAR = 2016\n",
    "TRAINING_START_YEAR = 1957\n",
    "VALIDATION_LENGTH_YEARS = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Setup and Framework ---\n",
      "Loading final dataset from '../data/gky_final_data.parquet'...\n",
      "Data loaded successfully. Shape: (2851604, 924)\n"
     ]
    }
   ],
   "source": [
    "# --- Part 1: Setup and Framework ---\n",
    "print(\"--- Part 1: Setup and Framework ---\")\n",
    "\n",
    "# Define file paths and constants\n",
    "INPUT_DIR = '../data'\n",
    "INPUT_PATH = os.path.join(INPUT_DIR, 'gky_final_data.parquet')\n",
    "\n",
    "# Load the final analysis-ready dataset\n",
    "print(f\"Loading final dataset from '{INPUT_PATH}'...\")\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(\"Final data file not found. Please run 01 and 02 notebooks first.\")\n",
    "gky_final_data = pd.read_parquet(INPUT_PATH)\n",
    "gky_final_data['month'] = pd.to_datetime(gky_final_data['month'])\n",
    "print(f\"Data loaded successfully. Shape: {gky_final_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation periods framework created.\n",
      "   oos_year  train_start  train_end  valid_start  valid_end\n",
      "0      1987         1957       1974         1975       1986\n",
      "1      1988         1957       1975         1976       1987\n",
      "2      1989         1957       1976         1977       1988\n",
      "Identified 920 predictor columns.\n"
     ]
    }
   ],
   "source": [
    "# Create the evaluation periods DataFrame\n",
    "periods = []\n",
    "for year in range(FIRST_OOS_YEAR, LAST_OOS_YEAR + 1):\n",
    "    periods.append({\n",
    "        'oos_year': year,\n",
    "        'train_start': TRAINING_START_YEAR,\n",
    "        'train_end': year - VALIDATION_LENGTH_YEARS - 1,\n",
    "        'valid_start': year - VALIDATION_LENGTH_YEARS,\n",
    "        'valid_end': year - 1,\n",
    "    })\n",
    "evaluation_periods_df = pd.DataFrame(periods)\n",
    "print(\"Evaluation periods framework created.\")\n",
    "print(evaluation_periods_df.head(3))\n",
    "\n",
    "# Define predictor columns once\n",
    "ID_COLS = ['permno', 'month', 'ret_excess', 'mktcap_lag']\n",
    "PREDICTOR_COLS = [col for col in gky_final_data.columns if col not in ID_COLS]\n",
    "print(f\"Identified {len(PREDICTOR_COLS)} predictor columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 2: Defining the Core `BaseModel` Engine ---\n",
      "BaseModel class defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Part 2: The Core Modeling Engine (`BaseModel` Class) ---\n",
    "print(\"\\n--- Part 2: Defining the Core `BaseModel` Engine ---\")\n",
    "\n",
    "class BaseModel:\n",
    "    \"\"\"\n",
    "    A generic base class for handling the GKY (2020) evaluation methodology.\n",
    "    \n",
    "    This class orchestrates the annual training, validation, and testing loop,\n",
    "    performs hyperparameter tuning using GridSearchCV with a PredefinedSplit,\n",
    "    and calculates the final out-of-sample R-squared.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, estimator: BaseEstimator, param_grid: dict, predictor_subset: list = None, is_huber: bool = False):\n",
    "        self.model_name = model_name\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.predictor_subset = predictor_subset if predictor_subset else PREDICTOR_COLS\n",
    "        self.all_oos_predictions = []\n",
    "        self.is_huber = is_huber\n",
    "        \n",
    "    def _train_and_predict_one_year(self, full_df: pd.DataFrame, period: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handles the model training, tuning, and prediction for a single OOS year.\n",
    "        WITH SPECIAL HANDLING FOR HUBER MODELS.\n",
    "        \"\"\"\n",
    "        # 1. Slice data (no change here)\n",
    "        train_df = full_df[(full_df['month'].dt.year >= period.train_start) & (full_df['month'].dt.year <= period.train_end)]\n",
    "        valid_df = full_df[(full_df['month'].dt.year >= period.valid_start) & (full_df['month'].dt.year <= period.valid_end)]\n",
    "        oos_df = full_df[full_df['month'].dt.year == period.oos_year]\n",
    "\n",
    "        # Combine train and validation for in-sample fitting\n",
    "        in_sample_df = pd.concat([train_df, valid_df], ignore_index=True)\n",
    "        X_in_sample, y_in_sample = in_sample_df[self.predictor_subset], in_sample_df['ret_excess']\n",
    "        X_oos, y_oos = oos_df[self.predictor_subset], oos_df['ret_excess']\n",
    "        \n",
    "        if self.is_huber:\n",
    "            # --- HUBER-SPECIFIC LOGIC ---\n",
    "            # Stage 1: Preliminary OLS on the entire in-sample data to get residuals\n",
    "            scaler = StandardScaler().fit(X_in_sample)\n",
    "            X_in_sample_scaled = scaler.transform(X_in_sample)\n",
    "            \n",
    "            prelim_ols = LinearRegression().fit(X_in_sample_scaled, y_in_sample)\n",
    "            residuals = y_in_sample - prelim_ols.predict(X_in_sample_scaled)\n",
    "            \n",
    "            # Stage 2: Calculate epsilon as per GKX\n",
    "            epsilon = np.quantile(np.abs(residuals), 0.999)\n",
    "            \n",
    "            # Stage 3: Fit the final HuberRegressor with the dynamic epsilon\n",
    "            best_model = HuberRegressor(epsilon=max(1.0, epsilon), max_iter=500).fit(X_in_sample_scaled, y_in_sample)\n",
    "            \n",
    "            # Prediction requires applying the same scaler\n",
    "            X_oos_scaled = scaler.transform(X_oos)\n",
    "            predictions = best_model.predict(X_oos_scaled)\n",
    "\n",
    "        else:\n",
    "            # --- STANDARD GridSearchCV LOGIC ---\n",
    "            # This part remains the same as your original code for non-Huber models\n",
    "            X_train, y_train = train_df[self.predictor_subset], train_df['ret_excess']\n",
    "            X_valid, y_valid = valid_df[self.predictor_subset], valid_df['ret_excess']\n",
    "            X_tune = pd.concat([X_train, X_valid], ignore_index=True)\n",
    "            y_tune = pd.concat([y_train, y_valid], ignore_index=True)\n",
    "            \n",
    "            split_index = [-1] * len(X_train) + [0] * len(X_valid)\n",
    "            pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "            tuner = GridSearchCV(\n",
    "                estimator=self.estimator, param_grid=self.param_grid,\n",
    "                scoring='neg_mean_squared_error', cv=pds, n_jobs=-1\n",
    "            )\n",
    "            tuner.fit(X_tune, y_tune)\n",
    "            best_model = tuner.best_estimator_\n",
    "            predictions = best_model.predict(X_oos)\n",
    "            \n",
    "        # 6. Store predictions (no change here)\n",
    "        oos_results = oos_df[ID_COLS].copy()\n",
    "        oos_results['prediction'] = predictions\n",
    "        return oos_results\n",
    "\n",
    "    def _calculate_oos_r2(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the OOS R-squared based on the paper's formula.\n",
    "        \"\"\"\n",
    "        if not self.all_oos_predictions:\n",
    "            raise ValueError(\"No OOS predictions found. `run_full_evaluation` must be called first.\")\n",
    "        \n",
    "        preds_df = pd.concat(self.all_oos_predictions, ignore_index=True)\n",
    "        \n",
    "        sse = ((preds_df['ret_excess'] - preds_df['prediction']) ** 2).sum()\n",
    "        sst = (preds_df['ret_excess'] ** 2).sum() # Note: Denominator is not variance\n",
    "        \n",
    "        if sst == 0: return -np.inf # Avoid division by zero\n",
    "        \n",
    "        r2_oos = 1 - (sse / sst)\n",
    "        return r2_oos * 100 # Return as a percentage\n",
    "\n",
    "    def run_full_evaluation(self, full_df: pd.DataFrame, periods_df: pd.DataFrame) -> tuple[float, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Executes the full evaluation loop over all specified periods.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Starting Full Evaluation for: {self.model_name} ---\")\n",
    "        self.all_oos_predictions = [] # Reset predictions\n",
    "        \n",
    "        for _, period in tqdm(periods_df.iterrows(), total=len(periods_df), desc=self.model_name):\n",
    "            try:\n",
    "                oos_preds_for_year = self._train_and_predict_one_year(full_df, period)\n",
    "                self.all_oos_predictions.append(oos_preds_for_year)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during {self.model_name} evaluation for OOS year {period.oos_year}: {e}\")\n",
    "            gc.collect() # Clean memory after each intensive year\n",
    "            \n",
    "        final_r2 = self._calculate_oos_r2()\n",
    "        print(f\"--- Evaluation Complete for: {self.model_name} | Final OOS R² = {final_r2:.4f}% ---\")\n",
    "        \n",
    "        return final_r2, pd.concat(self.all_oos_predictions, ignore_index=True)\n",
    "\n",
    "print(\"BaseModel class defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 3: Defining Specific Model Implementations ---\n",
      "All model classes defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Part 3: Model Definitions (Subclasses) ---\n",
    "print(\"\\n--- Part 3: Defining Specific Model Implementations ---\")\n",
    "\n",
    "# --- Linear Models ---\n",
    "class OLS3HModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        predictors = [\n",
    "            'characteristic_mvel1_x_macro_intercept', \n",
    "            'characteristic_bm_x_macro_intercept', \n",
    "            'characteristic_mom12m_x_macro_intercept'\n",
    "        ]\n",
    "        # Estimator and param_grid are now placeholders, as they are handled by the special logic\n",
    "        super().__init__(model_name=\"OLS-3+H\", estimator=None, param_grid={}, \n",
    "                         predictor_subset=predictors, is_huber=True)\n",
    "\n",
    "class OLSHModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        # Same placeholder logic\n",
    "        super().__init__(model_name=\"OLS+H\", estimator=None, param_grid={}, is_huber=True)\n",
    "        \n",
    "class PCRModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        # This class definition does not change\n",
    "        estimator = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('reg', LinearRegression())\n",
    "        ])\n",
    "        param_grid = {'pca__n_components': [10, 20, 30, 40, 50, 60]}\n",
    "        super().__init__(model_name=\"PCR\", estimator=estimator, param_grid=param_grid)\n",
    "\n",
    "class ENetHModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        # NOTE: scikit-learn's ElasticNet does not support Huber loss directly.\n",
    "        # This implementation uses the standard ElasticNet as the closest available\n",
    "        # off-the-shelf model, as allowed by the assignment. A full replication\n",
    "        # of the \"+H\" would require a different library.\n",
    "        estimator = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('reg', ElasticNet(l1_ratio=0.5, random_state=42, max_iter=1000))\n",
    "        ])\n",
    "        # Grid based on GKY Table A.5\n",
    "        param_grid = {'reg__alpha': np.logspace(-4, -1, 4)}\n",
    "        super().__init__(model_name=\"ENet+H\", estimator=estimator, param_grid=param_grid)\n",
    "\n",
    "# --- Tree-Based Models ---\n",
    "class RFModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        estimator = RandomForestRegressor(\n",
    "            n_estimators=300,\n",
    "            random_state=42,\n",
    "            n_jobs=-1 # Use all cores for tree building\n",
    "        )\n",
    "        # Grid based on GKY Table A.5 and TidyFinance post\n",
    "        param_grid = {\n",
    "            'max_features': [10, 20, 30, 50],\n",
    "            'min_samples_leaf': [5000, 10000] # Controls tree depth\n",
    "        }\n",
    "        super().__init__(model_name=\"RF\", estimator=estimator, param_grid=param_grid)\n",
    "\n",
    "# --- Neural Network Placeholders ---\n",
    "# These will be implemented using a deep learning library in the next step.\n",
    "class NN2Model(BaseModel):\n",
    "     def __init__(self):\n",
    "        # This is a placeholder and will not run correctly until implemented.\n",
    "        super().__init__(model_name=\"NN2\", estimator=None, param_grid={})\n",
    "\n",
    "class NN4Model(BaseModel):\n",
    "     def __init__(self):\n",
    "        # This is a placeholder and will not run correctly until implemented.\n",
    "        super().__init__(model_name=\"NN4\", estimator=None, param_grid={})\n",
    "\n",
    "print(\"All model classes defined successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 4: Running the Evaluation ---\n",
      "\n",
      "--- Starting Full Evaluation for: OLS-3+H ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OLS-3+H: 100%|██████████| 30/30 [12:18<00:00, 24.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Complete for: OLS-3+H | Final OOS R² = -0.5166% ---\n",
      "\n",
      "\n",
      "--- FINAL OOS R² RESULTS (Replication of Table 1) ---\n",
      "\n",
      "    OLS+H OLS-3+H  PCR ENet+H   RF\n",
      "All   N/A   -0.52  N/A    N/A  N/A\n",
      "\n",
      "Replication of scikit-learn based models complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Part 4: The Execution Block ---\n",
    "print(\"\\n--- Part 4: Running the Evaluation ---\")\n",
    "\n",
    "# NOTE: This block will take a significant amount of time to run,\n",
    "# especially for models with large hyperparameter grids.\n",
    "# It is recommended to run one or two models at a time to test.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Instantiate all models we want to evaluate for Table 1\n",
    "    models_to_run = {\n",
    "        # \"OLS+H\": OLSHModel(),\n",
    "        \"OLS-3+H\": OLS3HModel(),\n",
    "        # \"PCR\": PCRModel(),\n",
    "        # \"ENet+H\": ENetHModel(),\n",
    "        # \"RF\": RFModel(),\n",
    "        # \"NN2\": NN2Model(), # Uncomment when implemented\n",
    "        # \"NN4\": NN4Model(), # Uncomment when implemented\n",
    "    }\n",
    "    \n",
    "    final_results = {}\n",
    "\n",
    "    for name, model_obj in models_to_run.items():\n",
    "        try:\n",
    "            r2_score, oos_preds = model_obj.run_full_evaluation(gky_final_data, evaluation_periods_df)\n",
    "            final_results[name] = r2_score\n",
    "            \n",
    "            # Optional: Save the detailed predictions for later analysis (e.g., Figure 9)\n",
    "            # preds_output_path = os.path.join(INPUT_DIR, f'preds_{name.replace(\"+\", \"\")}.parquet')\n",
    "            # oos_preds.to_parquet(preds_output_path, index=False)\n",
    "            # print(f\"Saved predictions for {name} to {preds_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! FAILED EVALUATION FOR {name}: {e} !!!\")\n",
    "            final_results[name] = 'Failed'\n",
    "    \n",
    "    # --- Display Final Table 1 Results ---\n",
    "    print(\"\\n\\n--- FINAL OOS R² RESULTS (Replication of Table 1) ---\\n\")\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(final_results, orient='index', columns=['R2_OOS (%)'])\n",
    "    results_df.index.name = \"Model\"\n",
    "    \n",
    "    # Format the DataFrame to match the paper's style\n",
    "    table1_output = pd.DataFrame(index=['All'])\n",
    "    for model_name in [\"OLS+H\", \"OLS-3+H\", \"PCR\", \"ENet+H\", \"RF\"]: # Add NNs here later\n",
    "        if model_name in results_df.index:\n",
    "            table1_output[model_name] = f\"{results_df.loc[model_name, 'R2_OOS (%)']:.2f}\"\n",
    "        else:\n",
    "            table1_output[model_name] = \"N/A\"\n",
    "            \n",
    "    print(table1_output)\n",
    "    print(\"\\nReplication of scikit-learn based models complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 4: Running the Evaluation ---\n",
      "\n",
      "--- Starting Full Evaluation for: PCR ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCR:  37%|███▋      | 11/30 [33:06<1:03:04, 199.16s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "PCR:  60%|██████    | 18/30 [1:06:20<56:51, 284.25s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "PCR: 100%|██████████| 30/30 [2:21:36<00:00, 283.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Complete for: PCR | Final OOS R² = 0.2580% ---\n",
      "\n",
      "\n",
      "--- FINAL OOS R² RESULTS (Replication of Table 1) ---\n",
      "\n",
      "    OLS+H OLS-3+H   PCR ENet+H   RF\n",
      "All   N/A     N/A  0.26    N/A  N/A\n",
      "\n",
      "Replication of scikit-learn based models complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Part 4: The Execution Block ---\n",
    "print(\"\\n--- Part 4: Running the Evaluation ---\")\n",
    "\n",
    "# NOTE: This block will take a significant amount of time to run,\n",
    "# especially for models with large hyperparameter grids.\n",
    "# It is recommended to run one or two models at a time to test.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Instantiate all models we want to evaluate for Table 1\n",
    "    models_to_run = {\n",
    "        # \"OLS+H\": OLSHModel(),\n",
    "        # \"OLS-3+H\": OLS3HModel(),\n",
    "        \"PCR\": PCRModel(),\n",
    "        # \"ENet+H\": ENetHModel(),\n",
    "        # \"RF\": RFModel(),\n",
    "        # \"NN2\": NN2Model(), # Uncomment when implemented\n",
    "        # \"NN4\": NN4Model(), # Uncomment when implemented\n",
    "    }\n",
    "    \n",
    "    final_results = {}\n",
    "\n",
    "    for name, model_obj in models_to_run.items():\n",
    "        try:\n",
    "            r2_score, oos_preds = model_obj.run_full_evaluation(gky_final_data, evaluation_periods_df)\n",
    "            final_results[name] = r2_score\n",
    "            \n",
    "            # Optional: Save the detailed predictions for later analysis (e.g., Figure 9)\n",
    "            # preds_output_path = os.path.join(INPUT_DIR, f'preds_{name.replace(\"+\", \"\")}.parquet')\n",
    "            # oos_preds.to_parquet(preds_output_path, index=False)\n",
    "            # print(f\"Saved predictions for {name} to {preds_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! FAILED EVALUATION FOR {name}: {e} !!!\")\n",
    "            final_results[name] = 'Failed'\n",
    "    \n",
    "    # --- Display Final Table 1 Results ---\n",
    "    print(\"\\n\\n--- FINAL OOS R² RESULTS (Replication of Table 1) ---\\n\")\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(final_results, orient='index', columns=['R2_OOS (%)'])\n",
    "    results_df.index.name = \"Model\"\n",
    "    \n",
    "    # Format the DataFrame to match the paper's style\n",
    "    table1_output = pd.DataFrame(index=['All'])\n",
    "    for model_name in [\"OLS+H\", \"OLS-3+H\", \"PCR\", \"ENet+H\", \"RF\"]: # Add NNs here later\n",
    "        if model_name in results_df.index:\n",
    "            table1_output[model_name] = f\"{results_df.loc[model_name, 'R2_OOS (%)']:.2f}\"\n",
    "        else:\n",
    "            table1_output[model_name] = \"N/A\"\n",
    "            \n",
    "    print(table1_output)\n",
    "    print(\"\\nReplication of scikit-learn based models complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 4: Running the Evaluation ---\n",
      "\n",
      "--- Starting Full Evaluation for: ENet+H ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ENet+H:   0%|          | 0/30 [00:00<?, ?it/s]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.188e+02, tolerance: 5.150e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:   3%|▎         | 1/30 [09:29<4:35:06, 569.19s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.438e+02, tolerance: 6.827e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:   7%|▋         | 2/30 [20:16<4:47:01, 615.07s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.508e+02, tolerance: 7.914e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  10%|█         | 3/30 [32:44<5:04:09, 675.91s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.242e+02, tolerance: 8.618e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  13%|█▎        | 4/30 [45:45<5:10:46, 717.18s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.173e+00, tolerance: 9.678e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  17%|█▋        | 5/30 [1:00:14<5:21:40, 772.03s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.928e+01, tolerance: 1.066e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  20%|██        | 6/30 [1:16:00<5:32:31, 831.33s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.819e+01, tolerance: 1.208e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  23%|██▎       | 7/30 [1:32:54<5:41:33, 891.03s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.259e+01, tolerance: 1.321e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  27%|██▋       | 8/30 [1:51:20<5:51:49, 959.52s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.209e+01, tolerance: 1.479e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  30%|███       | 9/30 [2:11:24<6:02:35, 1035.96s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.098e+01, tolerance: 1.651e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  33%|███▎      | 10/30 [2:33:48<6:17:00, 1131.05s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.508e+01, tolerance: 1.785e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  37%|███▋      | 11/30 [2:58:24<6:31:32, 1236.45s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.967e+01, tolerance: 1.957e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  40%|████      | 12/30 [3:23:56<6:37:57, 1326.54s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+01, tolerance: 2.155e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  43%|████▎     | 13/30 [3:54:53<7:01:18, 1486.95s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.699e+01, tolerance: 2.411e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  47%|████▋     | 14/30 [4:34:40<7:49:05, 1759.07s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.990e+01, tolerance: 2.605e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  50%|█████     | 15/30 [5:12:38<7:58:49, 1915.30s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.355e+01, tolerance: 2.786e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  53%|█████▎    | 16/30 [5:53:34<8:04:53, 2078.07s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.642e+01, tolerance: 3.046e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  57%|█████▋    | 17/30 [6:41:54<8:23:49, 2325.32s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.134e+02, tolerance: 3.410e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  63%|██████▎   | 19/30 [8:11:41<7:41:33, 2517.63s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.056e+01, tolerance: 4.021e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  67%|██████▋   | 20/30 [9:07:21<7:40:45, 2764.53s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.837e+01, tolerance: 4.214e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  70%|███████   | 21/30 [10:06:42<7:30:31, 3003.49s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.024e+01, tolerance: 4.447e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  73%|███████▎  | 22/30 [11:06:31<7:03:53, 3179.22s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.114e+01, tolerance: 4.704e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  77%|███████▋  | 23/30 [12:07:08<6:26:56, 3316.60s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.066e+01, tolerance: 4.985e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  80%|████████  | 24/30 [13:11:52<5:48:41, 3486.90s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.616e+01, tolerance: 5.439e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  83%|████████▎ | 25/30 [14:15:22<4:58:40, 3584.03s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+01, tolerance: 5.933e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  87%|████████▋ | 26/30 [15:40:14<4:29:05, 4036.41s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.464e+01, tolerance: 6.479e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  90%|█████████ | 27/30 [16:58:13<3:31:27, 4229.27s/it]/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.882e+01, tolerance: 7.018e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "ENet+H:  93%|█████████▎| 28/30 [19:11:22<2:58:34, 5357.24s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Part 4: The Execution Block ---\n",
    "print(\"\\n--- Part 4: Running the Evaluation ---\")\n",
    "\n",
    "# NOTE: This block will take a significant amount of time to run,\n",
    "# especially for models with large hyperparameter grids.\n",
    "# It is recommended to run one or two models at a time to test.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Instantiate all models we want to evaluate for Table 1\n",
    "    models_to_run = {\n",
    "        # \"OLS+H\": OLSHModel(),\n",
    "        # \"OLS-3+H\": OLS3HModel(),\n",
    "        # \"PCR\": PCRModel(),\n",
    "        \"ENet+H\": ENetHModel(),\n",
    "        # \"RF\": RFModel(),\n",
    "        # \"NN2\": NN2Model(), # Uncomment when implemented\n",
    "        # \"NN4\": NN4Model(), # Uncomment when implemented\n",
    "    }\n",
    "    \n",
    "    final_results = {}\n",
    "\n",
    "    for name, model_obj in models_to_run.items():\n",
    "        try:\n",
    "            r2_score, oos_preds = model_obj.run_full_evaluation(gky_final_data, evaluation_periods_df)\n",
    "            final_results[name] = r2_score\n",
    "            \n",
    "            # Optional: Save the detailed predictions for later analysis (e.g., Figure 9)\n",
    "            # preds_output_path = os.path.join(INPUT_DIR, f'preds_{name.replace(\"+\", \"\")}.parquet')\n",
    "            # oos_preds.to_parquet(preds_output_path, index=False)\n",
    "            # print(f\"Saved predictions for {name} to {preds_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! FAILED EVALUATION FOR {name}: {e} !!!\")\n",
    "            final_results[name] = 'Failed'\n",
    "    \n",
    "    # --- Display Final Table 1 Results ---\n",
    "    print(\"\\n\\n--- FINAL OOS R² RESULTS (Replication of Table 1) ---\\n\")\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(final_results, orient='index', columns=['R2_OOS (%)'])\n",
    "    results_df.index.name = \"Model\"\n",
    "    \n",
    "    # Format the DataFrame to match the paper's style\n",
    "    table1_output = pd.DataFrame(index=['All'])\n",
    "    for model_name in [\"OLS+H\", \"OLS-3+H\", \"PCR\", \"ENet+H\", \"RF\"]: # Add NNs here later\n",
    "        if model_name in results_df.index:\n",
    "            table1_output[model_name] = f\"{results_df.loc[model_name, 'R2_OOS (%)']:.2f}\"\n",
    "        else:\n",
    "            table1_output[model_name] = \"N/A\"\n",
    "            \n",
    "    print(table1_output)\n",
    "    print(\"\\nReplication of scikit-learn based models complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gkx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
