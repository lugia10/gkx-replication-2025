{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GKY (2020) Replication: Part 2 - Data Merging & Final Matrix Construction\n",
    "\n",
    "##### This notebook takes the three prepared data files from `01_data_downloader.ipynb` and completes the data assembly process.\n",
    "\n",
    "##### 1. Step 4: Merging Data\n",
    "##### - We load `crsp_prepared.parquet`, `characteristics_prepared.parquet`, and `macro_predictors_lagged.parquet`.\n",
    "##### - We perform sequential `inner` joins to ensure that every row in our final dataset has a valid return, a full set of characteristics, and corresponding macro data for that month.\n",
    "\n",
    "##### 2.  **Step 5: Constructing the Final 920-Predictor Matrix**\n",
    "##### - Interaction Terms: We create interaction features by multiplying each of the 94 stock characteristics with each of the 8 macro predictors (plus a macro intercept), resulting in `94 * 9 = 846` interaction features. The original characteristics and macro variables are not kept, as per GKY's methodology.\n",
    "##### - Industry Dummies: We create 74 one-hot encoded dummy variables from the `sic2` industry codes.\n",
    "##### - Final Predictor Count: `846 (Interactions) + 74 (Dummies) = 920 Predictors`.\n",
    "\n",
    "##### The final output is a single Parquet file (`gky_final_data.parquet`) containing the response variable (`ret_excess`), identifying columns, and the complete 920-predictor matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required input files found.\n",
      "Loading prepared data from parquet files...\n",
      "Data successfully loaded.\n",
      "Characteristics shape: (3816941, 97)\n",
      "CRSP shape: (3065543, 4)\n",
      "Macro shape: (1836, 9)\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "# Define the input directory where the prepared files are stored.\n",
    "# This path is relative to this notebook's location in `notebooks/`.\n",
    "INPUT_DIR = '../data'\n",
    "\n",
    "# Define the final output file path.\n",
    "OUTPUT_PATH = os.path.join(INPUT_DIR, 'gky_final_data.parquet')\n",
    "\n",
    "# --- Verify Input Files Exist ---\n",
    "required_files = [\n",
    "    'characteristics_prepared.parquet',\n",
    "    'crsp_prepared.parquet',\n",
    "    'macro_predictors_lagged.parquet'\n",
    "]\n",
    "\n",
    "for f in required_files:\n",
    "    path = os.path.join(INPUT_DIR, f)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Required input file not found: {path}. \"\n",
    "            \"Please run the '01_data_downloader.ipynb' notebook first.\"\n",
    "        )\n",
    "print(\"All required input files found.\")\n",
    "\n",
    "# --- Load Prepared Data ---\n",
    "\n",
    "print(\"Loading prepared data from parquet files...\")\n",
    "characteristics_df = pd.read_parquet(os.path.join(INPUT_DIR, 'characteristics_prepared.parquet'))\n",
    "crsp_df = pd.read_parquet(os.path.join(INPUT_DIR, 'crsp_prepared.parquet'))\n",
    "macro_df = pd.read_parquet(os.path.join(INPUT_DIR, 'macro_predictors_lagged.parquet'))\n",
    "\n",
    "print(\"Data successfully loaded.\")\n",
    "print(f\"Characteristics shape: {characteristics_df.shape}\")\n",
    "print(f\"CRSP shape: {crsp_df.shape}\")\n",
    "print(f\"Macro shape: {macro_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Step 4: Merging DataFrames ---\n",
      "CRSP/Characteristics 'month' head:\n",
      "0   1986-03-31\n",
      "1   1986-04-30\n",
      "Name: month, dtype: datetime64[ns]\n",
      "Macro 'month' head (before fix):\n",
      "0   1871-02-28\n",
      "1   1871-03-28\n",
      "Name: month, dtype: datetime64[ns]\n",
      "------------------------------\n",
      "Applying date standardization fix to macro data...\n",
      "Macro 'month' head (after fix):\n",
      "0   1871-02-28\n",
      "1   1871-03-31\n",
      "Name: month, dtype: datetime64[ns]\n",
      "------------------------------\n",
      "Merging CRSP data with characteristics...\n",
      "Shape after merging with characteristics: (2851604, 99)\n",
      "Merging with lagged macroeconomic predictors...\n",
      "Final merged shape: (2851604, 107)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 4: Merge All Data Sources ---\n",
    "print(\"\\n--- Starting Step 4: Merging DataFrames ---\")\n",
    "\n",
    "# Check date conventions before fixing\n",
    "print(f\"CRSP/Characteristics 'month' head:\\n{crsp_df['month'].head(2)}\")\n",
    "print(f\"Macro 'month' head (before fix):\\n{macro_df['month'].head(2)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Standardize macro_df date to month-end IN-PLACE\n",
    "print(\"Applying date standardization fix to macro data...\")\n",
    "macro_df['month'] = macro_df['month'] + pd.offsets.MonthEnd(0)\n",
    "print(f\"Macro 'month' head (after fix):\\n{macro_df['month'].head(2)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# METHODOLOGY: Use 'inner' joins to ensure data integrity across all sources.\n",
    "# We only want observations that have returns, characteristics, and macro data.\n",
    "\n",
    "# Merge returns (CRSP) with stock-level characteristics\n",
    "print(\"Merging CRSP data with characteristics...\")\n",
    "merged_df = pd.merge(\n",
    "    crsp_df, \n",
    "    characteristics_df, \n",
    "    on=['permno', 'month'], \n",
    "    how='inner'\n",
    ")\n",
    "print(f\"Shape after merging with characteristics: {merged_df.shape}\")\n",
    "\n",
    "# Merge the result with macroeconomic predictors\n",
    "print(\"Merging with lagged macroeconomic predictors...\")\n",
    "merged_df = pd.merge(\n",
    "    merged_df, \n",
    "    macro_df, \n",
    "    on='month', \n",
    "    how='inner'\n",
    ")\n",
    "print(f\"Final merged shape: {merged_df.shape}\")\n",
    "\n",
    "# Memory Management: Clean up original dataframes\n",
    "del characteristics_df, crsp_df, macro_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Step 5: Constructing the Predictor Matrix ---\n",
      "Creating 74 industry dummies from 'sic2'...\n",
      "Temporarily reloading characteristics data to get all SIC codes...\n",
      "Identified 74 unique SIC codes from the source file.\n",
      "Shape of industry dummies: (2851604, 74)\n",
      "SUCCESS: Correctly created 74 industry dummy columns.\n",
      "Creating 846 interaction features (this is computationally intensive)...\n",
      "Found 94 characteristic columns.\n",
      "Found 9 macro columns (including intercept).\n",
      "Shape of interaction features: (2851604, 846)\n",
      "Assembling the final analysis-ready DataFrame...\n",
      "--- Final Matrix Construction Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Construct the Final 920-Predictor Matrix ---\n",
    "\n",
    "print(\"\\n--- Starting Step 5: Constructing the Predictor Matrix ---\")\n",
    "\n",
    "# --- Part A: Create Industry Dummies ---\n",
    "print(\"Creating 74 industry dummies from 'sic2'...\")\n",
    "\n",
    "# To ensure exactly 74 dummies are created, we first identify all possible SIC codes from the original, un-merged characteristics data.\n",
    "print(\"Temporarily reloading characteristics data to get all SIC codes...\")\n",
    "original_chars_df = pd.read_parquet(os.path.join(INPUT_DIR, 'characteristics_prepared.parquet'))\n",
    "all_sic_codes = sorted(original_chars_df['sic2'].unique())\n",
    "del original_chars_df # Clean up memory\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Identified {len(all_sic_codes)} unique SIC codes from the source file.\")\n",
    "\n",
    "# Convert the 'sic2' column in our final merged data to a 'categorical' type.\n",
    "# By explicitly providing all possible categories, we force get_dummies to create a column for each, even if a category is missing after the merge.\n",
    "merged_df['sic2'] = pd.Categorical(merged_df['sic2'], categories=all_sic_codes)\n",
    "\n",
    "# Now, create the dummies. This will generate 74 columns.\n",
    "sic2_dummies = pd.get_dummies(merged_df['sic2'], prefix='sic2', dtype=float)\n",
    "\n",
    "print(f\"Shape of industry dummies: {sic2_dummies.shape}\")\n",
    "if sic2_dummies.shape[1] == 74:\n",
    "    print(\"SUCCESS: Correctly created 74 industry dummy columns.\")\n",
    "else:\n",
    "    print(f\"WARNING: Expected 74 SIC dummies, but got {sic2_dummies.shape[1]}.\")\n",
    "\n",
    "# --- Part B: Create Interaction Terms ---\n",
    "print(\"Creating 846 interaction features (this is computationally intensive)...\")\n",
    "\n",
    "# METHODOLOGY: Interact 94 characteristics with 8 macro predictors + 1 intercept.\n",
    "# Add the intercept column to the macro predictors before creating interactions.\n",
    "merged_df['macro_intercept'] = 1.0\n",
    "\n",
    "# Identify the column groups\n",
    "char_cols = [col for col in merged_df.columns if 'characteristic_' in col]\n",
    "macro_cols = [col for col in merged_df.columns if 'macro_' in col]\n",
    "\n",
    "print(f\"Found {len(char_cols)} characteristic columns.\")\n",
    "print(f\"Found {len(macro_cols)} macro columns (including intercept).\")\n",
    "\n",
    "# Use a highly efficient vectorized approach instead of a slow loop\n",
    "char_matrix = merged_df[char_cols].values\n",
    "macro_matrix = merged_df[macro_cols].values\n",
    "\n",
    "# Broadcasting creates all pairwise products efficiently:\n",
    "# (N, 94, 1) * (N, 1, 9) -> (N, 94, 9)\n",
    "interaction_matrix = char_matrix[:, :, np.newaxis] * macro_matrix[:, np.newaxis, :]\n",
    "# Reshape to the final (N, 846) matrix\n",
    "interaction_matrix_reshaped = interaction_matrix.reshape(interaction_matrix.shape[0], -1)\n",
    "\n",
    "# Create meaningful column names for the new features\n",
    "interaction_col_names = [\n",
    "    f\"{c_col}_x_{m_col}\" for c_col in char_cols for m_col in macro_cols\n",
    "]\n",
    "\n",
    "# Convert the NumPy array back to a DataFrame\n",
    "interaction_df = pd.DataFrame(\n",
    "    interaction_matrix_reshaped,\n",
    "    columns=interaction_col_names,\n",
    "    index=merged_df.index\n",
    ")\n",
    "print(f\"Shape of interaction features: {interaction_df.shape}\")\n",
    "\n",
    "# --- Part C: Assemble the Final DataFrame ---\n",
    "print(\"Assembling the final analysis-ready DataFrame...\")\n",
    "\n",
    "# Identify ID and response columns to keep\n",
    "id_response_cols = ['permno', 'month', 'ret_excess', 'mktcap_lag']\n",
    "final_ids_df = merged_df[id_response_cols]\n",
    "\n",
    "# Concatenate the final pieces: IDs, Dummies, and Interactions\n",
    "final_df = pd.concat([final_ids_df, sic2_dummies, interaction_df], axis=1)\n",
    "\n",
    "# Memory Management\n",
    "del merged_df, sic2_dummies, interaction_df, final_ids_df\n",
    "del char_matrix, macro_matrix, interaction_matrix, interaction_matrix_reshaped\n",
    "gc.collect()\n",
    "\n",
    "print(\"--- Final Matrix Construction Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verifying the Final DataFrame ---\n",
      "Final DataFrame shape: (2851604, 924)\n",
      "Total number of predictor columns: 920\n",
      "SUCCESS: The number of predictors (920) matches the GKY paper.\n",
      "\n",
      "Final DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2851604 entries, 0 to 2851603\n",
      "Columns: 924 entries, permno to characteristic_zerotrade_x_macro_intercept\n",
      "dtypes: datetime64[ns](1), float32(1), float64(921), int32(1)\n",
      "memory usage: 19.6 GB\n",
      "\n",
      "Head of the final DataFrame:\n",
      "   permno      month  ret_excess   mktcap_lag  sic2_1.0  sic2_2.0  sic2_7.0  \\\n",
      "0   10000 1986-03-31    0.359385  11960.00000       0.0       0.0       0.0   \n",
      "1   10000 1986-04-30   -0.103792  16330.00000       0.0       0.0       0.0   \n",
      "2   10000 1986-05-31   -0.227556  15172.00000       0.0       0.0       0.0   \n",
      "3   10000 1986-06-30   -0.010225  11793.87834       0.0       0.0       0.0   \n",
      "4   10000 1986-07-31   -0.086008  11734.59375       0.0       0.0       0.0   \n",
      "\n",
      "   sic2_8.0  sic2_9.0  sic2_10.0  ...  \\\n",
      "0       0.0       0.0        0.0  ...   \n",
      "1       0.0       0.0        0.0  ...   \n",
      "2       0.0       0.0        0.0  ...   \n",
      "3       0.0       0.0        0.0  ...   \n",
      "4       0.0       0.0        0.0  ...   \n",
      "\n",
      "   characteristic_std_turn_x_macro_intercept  \\\n",
      "0                                  -0.418432   \n",
      "1                                  -0.131466   \n",
      "2                                  -0.203288   \n",
      "3                                  -0.024084   \n",
      "4                                  -0.309073   \n",
      "\n",
      "   characteristic_zerotrade_x_macro_dp  characteristic_zerotrade_x_macro_ep  \\\n",
      "0                            -0.516450                            -0.423786   \n",
      "1                            -0.486041                            -0.401038   \n",
      "2                            -0.560824                            -0.462062   \n",
      "3                            -0.115121                            -0.095084   \n",
      "4                            -1.645509                            -1.359511   \n",
      "\n",
      "   characteristic_zerotrade_x_macro_bm  characteristic_zerotrade_x_macro_ntis  \\\n",
      "0                             0.082748                              -0.002764   \n",
      "1                             0.074411                              -0.002351   \n",
      "2                             0.087983                              -0.004083   \n",
      "3                             0.016939                              -0.000736   \n",
      "4                             0.239319                              -0.005892   \n",
      "\n",
      "   characteristic_zerotrade_x_macro_tbl  characteristic_zerotrade_x_macro_tms  \\\n",
      "0                              0.010892                              0.002083   \n",
      "1                              0.009394                              0.001575   \n",
      "2                              0.010065                              0.002923   \n",
      "3                              0.002069                              0.000784   \n",
      "4                              0.029766                              0.008101   \n",
      "\n",
      "   characteristic_zerotrade_x_macro_dfy  \\\n",
      "0                              0.002222   \n",
      "1                              0.002148   \n",
      "2                              0.002325   \n",
      "3                              0.000404   \n",
      "4                              0.005800   \n",
      "\n",
      "   characteristic_zerotrade_x_macro_svar  \\\n",
      "0                               0.000168   \n",
      "1                               0.000197   \n",
      "2                               0.000408   \n",
      "3                               0.000046   \n",
      "4                               0.000765   \n",
      "\n",
      "   characteristic_zerotrade_x_macro_intercept  \n",
      "0                                    0.154272  \n",
      "1                                    0.143201  \n",
      "2                                    0.166095  \n",
      "3                                    0.033639  \n",
      "4                                    0.479328  \n",
      "\n",
      "[5 rows x 924 columns]\n",
      "\n",
      "Saving final analysis-ready data to '../data/gky_final_data.parquet'...\n",
      "Save complete. The data is now ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "# --- Final Verification and Saving ---\n",
    "\n",
    "print(\"\\n--- Verifying the Final DataFrame ---\")\n",
    "\n",
    "# Check the final shape\n",
    "print(f\"Final DataFrame shape: {final_df.shape}\")\n",
    "\n",
    "# Verify the number of predictors\n",
    "predictor_cols = [col for col in final_df.columns if col not in id_response_cols]\n",
    "num_predictors = len(predictor_cols)\n",
    "print(f\"Total number of predictor columns: {num_predictors}\")\n",
    "\n",
    "if num_predictors == 920:\n",
    "    print(\"SUCCESS: The number of predictors (920) matches the GKY paper.\")\n",
    "else:\n",
    "    print(f\"WARNING: The number of predictors ({num_predictors}) does NOT match the GKY paper (920). Please review the steps.\")\n",
    "\n",
    "# Display info and head\n",
    "print(\"\\nFinal DataFrame Info:\")\n",
    "final_df.info(verbose=False, memory_usage='deep')\n",
    "\n",
    "print(\"\\nHead of the final DataFrame:\")\n",
    "print(final_df.head())\n",
    "\n",
    "print(f\"\\nSaving final analysis-ready data to '{OUTPUT_PATH}'...\")\n",
    "final_df.to_parquet(OUTPUT_PATH, index=False)\n",
    "print(\"Save complete. The data is now ready for modeling.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gkx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
