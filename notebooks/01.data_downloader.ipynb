{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GKY (2020) Replication: Part 1 - Data Collection & Preparation\n",
    "\n",
    "##### This notebook handles the first three major steps of the data pipeline:\n",
    "##### 1.  **Prepare Characteristics Data:** Downloads, cleans, rank-transforms, and imputes the 94 stock-level characteristics from Prof. Xiu's website.\n",
    "##### 2.  **Prepare CRSP Data:** Fetches stock returns from WRDS, applies GKY filters, and calculates excess returns and lagged market cap.\n",
    "##### 3.  **Prepare Macro Data:** Downloads and lags the 8 Welch-Goyal macroeconomic predictors.\n",
    "\n",
    "##### All prepared data is saved to the top-level `/data` directory in the efficient Parquet format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ssl\n",
    "import io\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import wrds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sample period as per the GKY paper\n",
    "START_DATE = '1957-03-01'\n",
    "END_DATE = '2016-12-31'\n",
    "\n",
    "# List of the 8 specific macroeconomic predictors required by GKY (2020)\n",
    "GKY_MACRO_PREDICTORS = [\n",
    "    'dp', 'ep', 'bm', 'ntis', 'tbl', 'tms', 'dfy', 'svar'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "OUTPUT_DIR = '../data' \n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# Define final file paths in one place\n",
    "CHARACTERISTICS_PATH = os.path.join(OUTPUT_DIR, 'characteristics_prepared.parquet')\n",
    "CRSP_PATH = os.path.join(OUTPUT_DIR, 'crsp_prepared.parquet')\n",
    "MACRO_PATH = os.path.join(OUTPUT_DIR, 'macro_predictors_lagged.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Prepare Stock-Level Characteristics ---\n",
    "\n",
    "def _rank_transform(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Internal helper to perform cross-sectional rank transformation on a [-1, 1] interval.\n",
    "    \"\"\"\n",
    "    n_valid = series.notna().sum()\n",
    "    if n_valid <= 1:\n",
    "        return series\n",
    "    ranked_series = series.rank(na_option='keep')\n",
    "    return 2 * ((ranked_series - 1) / (n_valid - 1) - 0.5)\n",
    "\n",
    "def prepare_characteristics_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Acquires and pre-processes the 94 GKY stock-level characteristics.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Step 1: Prepare Stock-Level Characteristics ---\")\n",
    "    \n",
    "    print(\"Downloading characteristics data from Dacheng Xiu's website...\")\n",
    "    url = \"https://dachxiu.chicagobooth.edu/download/datashare.zip\"\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        with z.open('datashare.csv') as f:\n",
    "            df = pd.read_csv(f)\n",
    "    print(f\"Successfully downloaded and read {len(df)} rows.\")\n",
    "    \n",
    "    df.rename(columns={'DATE': 'month'}, inplace=True)\n",
    "    df['month'] = pd.to_datetime(df['month'], format='%Y%m%d') + pd.offsets.MonthEnd(0)\n",
    "    \n",
    "    char_cols = [col for col in df.columns if col not in ['permno', 'month', 'sic2']]\n",
    "    rename_map = {col: f\"characteristic_{col}\" for col in char_cols}\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    df.dropna(subset=['sic2'], inplace=True)\n",
    "    df['sic2'] = df['sic2'].astype('category')\n",
    "    \n",
    "    char_cols_prefixed = list(rename_map.values())\n",
    "    \n",
    "    print(\"Applying cross-sectional rank transformation...\")\n",
    "    df[char_cols_prefixed] = df.groupby('month')[char_cols_prefixed].transform(_rank_transform)\n",
    "    \n",
    "    print(\"Imputing missing values (two-step: median then zero)...\")\n",
    "    df[char_cols_prefixed] = df.groupby('month')[char_cols_prefixed].transform(lambda x: x.fillna(x.median()))\n",
    "    df[char_cols_prefixed] = df[char_cols_prefixed].fillna(0)\n",
    "    \n",
    "    final_df = df[['permno', 'month', 'sic2'] + char_cols_prefixed].reset_index(drop=True)\n",
    "    \n",
    "    final_df['permno'] = final_df['permno'].astype('int32')\n",
    "    for col in char_cols_prefixed:\n",
    "        final_df[col] = final_df[col].astype('float32')\n",
    "        \n",
    "    gc.collect()\n",
    "\n",
    "    print(\"--- Characteristics Data Preparation Complete ---\")\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Prepare CRSP Return Data ---\n",
    "\n",
    "def prepare_crsp_data(db: wrds.Connection) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches and prepares monthly CRSP stock data as per GKY (2020) methodology.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Step 2: Prepare CRSP Return Data ---\")\n",
    "\n",
    "    print(\"Fetching CRSP monthly stock data (crsp.msf)...\")\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            a.permno, a.date, a.ret, a.prc, a.shrout,\n",
    "            b.shrcd, b.exchcd\n",
    "        FROM \n",
    "            crsp.msf AS a\n",
    "        LEFT JOIN \n",
    "            crsp.msenames AS b \n",
    "            ON a.permno = b.permno AND b.namedt <= a.date AND a.date <= b.nameendt\n",
    "        WHERE \n",
    "            a.date BETWEEN '{START_DATE}' AND '{END_DATE}'\n",
    "            AND b.shrcd IN (10, 11)\n",
    "            AND b.exchcd IN (1, 2, 3)\n",
    "    \"\"\"\n",
    "    crsp_df = db.raw_sql(sql_query, date_cols=['date'])\n",
    "    \n",
    "    print(\"Fetching Fama-French risk-free rate...\")\n",
    "    ff_df = db.raw_sql(\n",
    "        f\"SELECT date, rf FROM ff.factors_monthly WHERE date BETWEEN '{START_DATE}' AND '{END_DATE}'\", \n",
    "        date_cols=['date']\n",
    "    )\n",
    "    \n",
    "    print(\"Merging, cleaning, and calculating variables...\")\n",
    "    crsp_df['month'] = crsp_df['date'] + pd.offsets.MonthEnd(0)\n",
    "    ff_df['month'] = ff_df['date'] + pd.offsets.MonthEnd(0)\n",
    "    \n",
    "    df = pd.merge(crsp_df, ff_df[['month', 'rf']], on='month', how='left')\n",
    "    df['ret'] = pd.to_numeric(df['ret'], errors='coerce')\n",
    "    df.dropna(subset=['ret', 'rf', 'prc', 'shrout'], inplace=True)\n",
    "\n",
    "    df['mktcap'] = np.abs(df['prc']) * df['shrout']\n",
    "    df.sort_values(by=['permno', 'month'], inplace=True)\n",
    "    df['mktcap_lag'] = df.groupby('permno')['mktcap'].shift(1)\n",
    "    df['ret_excess'] = df['ret'] - df['rf']\n",
    "    \n",
    "    df.dropna(subset=['mktcap_lag', 'ret_excess'], inplace=True)\n",
    "\n",
    "    final_df = df[['permno', 'month', 'ret_excess', 'mktcap_lag']].reset_index(drop=True)\n",
    "    final_df = final_df.astype({\n",
    "        'permno': 'int32', 'ret_excess': 'float32', 'mktcap_lag': 'float64'\n",
    "    })\n",
    "\n",
    "    del crsp_df, ff_df, df\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"--- CRSP Data Preparation Complete ---\")\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Prepare Macroeconomic Predictors ---\n",
    "\n",
    "def prepare_macro_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads and prepares the 8 Welch-Goyal macroeconomic predictors.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Step 3: Prepare Macroeconomic Predictors ---\")\n",
    "    \n",
    "    sheet_id = \"1bM7vCWd3WOt95Sf9qjLPZjoiafgF_8EG\"\n",
    "    sheet_name = \"macro_predictors.xlsx\"\n",
    "    macro_link = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "    \n",
    "    print(f\"Acquiring macro data from TidyFinance source...\")\n",
    "    response = requests.get(macro_link)\n",
    "    response.raise_for_status()\n",
    "    macro_raw = pd.read_csv(io.StringIO(response.text), thousands=\",\")\n",
    "        \n",
    "    macro_processed = (\n",
    "        macro_raw\n",
    "        .assign(\n",
    "            month=lambda x: pd.to_datetime(x[\"yyyymm\"], format=\"%Y%m\") + pd.offsets.MonthEnd(0),\n",
    "            dp=lambda x: np.log(x[\"D12\"]) - np.log(x[\"Index\"]),\n",
    "            ep=lambda x: np.log(x[\"E12\"]) - np.log(x[\"Index\"]),\n",
    "            tms=lambda x: x[\"lty\"] - x[\"tbl\"],\n",
    "            dfy=lambda x: x[\"BAA\"] - x[\"AAA\"]\n",
    "        )\n",
    "        .rename(columns={\"b/m\": \"bm\"})\n",
    "        [['month'] + GKY_MACRO_PREDICTORS]\n",
    "    )\n",
    "    \n",
    "    macro_lagged = macro_processed.copy()\n",
    "    macro_lagged['month'] = macro_lagged['month'] + pd.DateOffset(months=1)\n",
    "    \n",
    "    rename_map = {col: f\"macro_{col}\" for col in GKY_MACRO_PREDICTORS}\n",
    "    macro_lagged.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    del macro_raw, macro_processed\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"--- Macro Data Preparation Complete ---\")\n",
    "    return macro_lagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to WRDS...\n",
      "Loading library list...\n",
      "Done\n",
      "WRDS Connection Successful.\n",
      "\n",
      "--- Starting Step 1: Prepare Stock-Level Characteristics ---\n",
      "Downloading characteristics data from Dacheng Xiu's website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/home/lugia10/.conda/envs/gkx_env/lib/python3.10/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'dachxiu.chicagobooth.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded and read 4117300 rows.\n",
      "Applying cross-sectional rank transformation...\n",
      "Imputing missing values (two-step: median then zero)...\n",
      "--- Characteristics Data Preparation Complete ---\n",
      "\n",
      "--- Starting Step 2: Prepare CRSP Return Data ---\n",
      "Fetching CRSP monthly stock data (crsp.msf)...\n",
      "Fetching Fama-French risk-free rate...\n",
      "Merging, cleaning, and calculating variables...\n",
      "--- CRSP Data Preparation Complete ---\n",
      "\n",
      "--- Starting Step 3: Prepare Macroeconomic Predictors ---\n",
      "Acquiring macro data from TidyFinance source...\n",
      "--- Macro Data Preparation Complete ---\n",
      "\n",
      "Saving prepared files to '../data' directory...\n",
      "All files saved successfully.\n",
      "\n",
      "WRDS Connection Closed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    db = None\n",
    "    try:\n",
    "        # Step 0: Connect to WRDS\n",
    "        print(\"Connecting to WRDS...\")\n",
    "        db = wrds.Connection()\n",
    "        print(\"WRDS Connection Successful.\")\n",
    "        \n",
    "        # Execute data preparation steps\n",
    "        characteristics_df = prepare_characteristics_data()\n",
    "        crsp_df = prepare_crsp_data(db)\n",
    "        macro_df = prepare_macro_data()\n",
    "        \n",
    "        # Save the final prepared data\n",
    "        print(f\"\\nSaving prepared files to '{OUTPUT_DIR}' directory...\")\n",
    "        characteristics_df.to_parquet(CHARACTERISTICS_PATH, index=False)\n",
    "        crsp_df.to_parquet(CRSP_PATH, index=False)\n",
    "        macro_df.to_parquet(MACRO_PATH, index=False)\n",
    "        print(\"All files saved successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the connection is always closed\n",
    "        if db:\n",
    "            db.close()\n",
    "            print(\"\\nWRDS Connection Closed.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (OPTIONAL) The below is to read and check the stored dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration ---\n",
    "# # Define the directory where your data is stored\n",
    "# DATA_DIR = '../data' \n",
    "\n",
    "# # --- Load the Prepared DataFrames ---\n",
    "# print(\"Loading prepared data from parquet files...\")\n",
    "\n",
    "# # Define file paths\n",
    "# characteristics_path = os.path.join(DATA_DIR, 'characteristics_prepared.parquet')\n",
    "# crsp_path = os.path.join(DATA_DIR, 'crsp_prepared.parquet')\n",
    "# macro_path = os.path.join(DATA_DIR, 'macro_predictors_lagged.parquet')\n",
    "\n",
    "# # Check if files exist before trying to read them\n",
    "# if not os.path.exists(crsp_path) or not os.path.exists(macro_path):\n",
    "#     print(\"Error: One or more parquet files not found.\")\n",
    "#     print(\"Please run the data preparation script first.\")\n",
    "#     # Exit or handle the error as needed\n",
    "\n",
    "# # Read the parquet files into pandas DataFrames\n",
    "# characteristics_df = pd.read_parquet(characteristics_path)\n",
    "# crsp_df = pd.read_parquet(crsp_path)\n",
    "# macro_df = pd.read_parquet(macro_path)\n",
    "\n",
    "# print(\"Data successfully loaded.\")\n",
    "\n",
    "# # --- Verify the Loaded Data ---\n",
    "# print(\"\\n--- Characteristics Data Information ---\")\n",
    "# print(characteristics_df.info())\n",
    "# print(\"\\nHead of Characteristics data:\")\n",
    "# print(characteristics_df.head())\n",
    "\n",
    "# print(\"\\n--- CRSP Data Information ---\")\n",
    "# print(crsp_df.info())\n",
    "# print(\"\\nHead of CRSP data:\")\n",
    "# print(crsp_df.head())\n",
    "\n",
    "# print(\"\\n--- Macro Data Information ---\")\n",
    "# print(macro_df.info())\n",
    "# print(\"\\nHead of Macro data:\")\n",
    "# print(macro_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gkx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
